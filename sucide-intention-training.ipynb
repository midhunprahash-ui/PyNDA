{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12572656,"sourceType":"datasetVersion","datasetId":7939883}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport pandas as pd\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import BertTokenizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport torch.nn as nn\nfrom transformers import BertModel","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-25T08:28:20.368367Z","iopub.execute_input":"2025-07-25T08:28:20.368876Z","iopub.status.idle":"2025-07-25T08:28:57.034778Z","shell.execute_reply.started":"2025-07-25T08:28:20.368853Z","shell.execute_reply":"2025-07-25T08:28:57.034194Z"}},"outputs":[{"name":"stderr","text":"2025-07-25 08:28:41.296387: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1753432121.661152      78 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1753432121.771573      78 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"class MultimodalSuicideRiskModel(nn.Module):\n    def __init__(self, bert_model_name='bert-base-uncased', n_structured_features=4):\n        super(MultimodalSuicideRiskModel, self).__init__()\n        self.bert = BertModel.from_pretrained(bert_model_name)\n        self.bert_output_size = self.bert.config.hidden_size\n        self.structured_mlp = nn.Sequential(\n            nn.Linear(n_structured_features, 32),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(32, 16),\n            nn.ReLU(),\n            nn.Dropout(0.2)\n        )\n        self.mlp_output_size = 16\n        combined_feature_size = self.bert_output_size + self.mlp_output_size\n        self.prediction_head = nn.Sequential(\n            nn.Linear(combined_feature_size, 64),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(64, 1)\n        )\n\n    def forward(self, input_ids, attention_mask, structured_data):\n        bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        text_features = bert_output.pooler_output\n        structured_features = self.structured_mlp(structured_data)\n        combined_features = torch.cat((text_features, structured_features), dim=1)\n        final_prediction = self.prediction_head(combined_features)\n        return final_prediction","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T08:28:57.036244Z","iopub.execute_input":"2025-07-25T08:28:57.036806Z","iopub.status.idle":"2025-07-25T08:28:57.043774Z","shell.execute_reply.started":"2025-07-25T08:28:57.036785Z","shell.execute_reply":"2025-07-25T08:28:57.043052Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"class SuicideDataset(Dataset):\n    def __init__(self, dataframe, tokenizer, max_len, scaler):\n        self.tokenizer = tokenizer\n        self.data = dataframe\n        self.text = dataframe.user_text\n        self.targets = dataframe.intention_score\n        # Scale the age column\n        self.data['age_scaled'] = scaler.transform(self.data[['age']])\n        self.structured = self.data[['age_scaled', 'gender_male', 'gender_female', 'gender_non_binary']].values\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.text)\n\n    def __getitem__(self, index):\n        text = str(self.text[index])\n\n        # Tokenize the text\n        inputs = self.tokenizer.encode_plus(\n            text,\n            None,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            padding='max_length',\n            return_token_type_ids=False,\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt'\n        )\n\n        # Return a dictionary of all the required data\n        return {\n            'input_ids': inputs['input_ids'].flatten(),\n            'attention_mask': inputs['attention_mask'].flatten(),\n            'structured_data': torch.tensor(self.structured[index], dtype=torch.float),\n            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n        }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T08:28:57.044660Z","iopub.execute_input":"2025-07-25T08:28:57.044944Z","iopub.status.idle":"2025-07-25T08:28:57.088363Z","shell.execute_reply.started":"2025-07-25T08:28:57.044920Z","shell.execute_reply":"2025-07-25T08:28:57.087680Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# --- 2. Load and Prepare the Data ---\ndf = pd.read_csv('/kaggle/input/sucide-intention-dataset/suicide_intention_dataset.csv')\n\n# Split data into train, validation, and test sets (70-15-15 split)\ntrain_df, temp_df = train_test_split(df, test_size=0.3, random_state=42)\nval_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n\n# --- THE FIX IS HERE ---\n# Reset the index of each dataframe to prevent KeyErrors\ntrain_df = train_df.reset_index(drop=True)\nval_df = val_df.reset_index(drop=True)\ntest_df = test_df.reset_index(drop=True)\n# ---------------------\n\n# It's good practice to scale numerical features like age\nage_scaler = StandardScaler()\nage_scaler.fit(train_df[['age']]) # Fit ONLY on the training data\n\n# --- 3. Set Up Tokenizer and DataLoaders ---\n# (This part remains the same)\nMAX_LEN = 160\nTRAIN_BATCH_SIZE = 16\nVAL_BATCH_SIZE = 16\nTOKENIZER = BertTokenizer.from_pretrained('bert-base-uncased')\n\ntrain_dataset = SuicideDataset(train_df, TOKENIZER, MAX_LEN, age_scaler)\nval_dataset = SuicideDataset(val_df, TOKENIZER, MAX_LEN, age_scaler)\ntest_dataset = SuicideDataset(test_df, TOKENIZER, MAX_LEN, age_scaler)\n\ntrain_loader = DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=VAL_BATCH_SIZE, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=VAL_BATCH_SIZE, shuffle=False)\n\nprint(f\"Train set size: {len(train_df)}\")\nprint(f\"Validation set size: {len(val_df)}\")\nprint(f\"Test set size: {len(test_df)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T08:28:57.089368Z","iopub.execute_input":"2025-07-25T08:28:57.089670Z","iopub.status.idle":"2025-07-25T08:28:58.352084Z","shell.execute_reply.started":"2025-07-25T08:28:57.089647Z","shell.execute_reply":"2025-07-25T08:28:58.351508Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5024ff5cb582410cb90df9da854178a9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aed20956f2b24b3b9340e6308942e873"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6149f2c7aa23422da8ce4b4b9ef84a02"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31512d0f36da4fd99abc9a1ce848df75"}},"metadata":{}},{"name":"stdout","text":"Train set size: 5600\nValidation set size: 1200\nTest set size: 1200\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# --- 4. Define the Training and Validation Functions ---\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\ndef train_epoch(model, data_loader, loss_fn, optimizer, device):\n    model = model.train()\n    total_loss = 0\n\n    for d in data_loader:\n        # Move data to the selected device (GPU/CPU)\n        input_ids = d[\"input_ids\"].to(device, dtype=torch.long)\n        attention_mask = d[\"attention_mask\"].to(device, dtype=torch.long)\n        structured_data = d[\"structured_data\"].to(device, dtype=torch.float)\n        targets = d[\"targets\"].to(device, dtype=torch.float)\n\n        # Get model outputs\n        outputs = model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            structured_data=structured_data\n        )\n\n        # Calculate loss\n        loss = loss_fn(outputs.squeeze(), targets)\n        total_loss += loss.item()\n\n        # Backward pass and optimization\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n    return total_loss / len(data_loader)\n\ndef eval_model(model, data_loader, loss_fn, device):\n    model = model.eval()\n    total_loss = 0\n\n    with torch.no_grad():\n        for d in data_loader:\n            input_ids = d[\"input_ids\"].to(device, dtype=torch.long)\n            attention_mask = d[\"attention_mask\"].to(device, dtype=torch.long)\n            structured_data = d[\"structured_data\"].to(device, dtype=torch.float)\n            targets = d[\"targets\"].to(device, dtype=torch.float)\n\n            outputs = model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                structured_data=structured_data\n            )\n\n            loss = loss_fn(outputs.squeeze(), targets)\n            total_loss += loss.item()\n\n    return total_loss / len(data_loader)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T08:28:58.353425Z","iopub.execute_input":"2025-07-25T08:28:58.353713Z","iopub.status.idle":"2025-07-25T08:28:58.360962Z","shell.execute_reply.started":"2025-07-25T08:28:58.353695Z","shell.execute_reply":"2025-07-25T08:28:58.360291Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# --- 5. Run the Training ---\nEPOCHS = 10\nmodel = MultimodalSuicideRiskModel().to(DEVICE)\noptimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\nloss_fn = nn.MSELoss() # Mean Squared Error is good for regression\n\nbest_val_loss = float('inf')\n\nfor epoch in range(EPOCHS):\n    print(f'Epoch {epoch + 1}/{EPOCHS}')\n    print('-' * 10)\n\n    train_loss = train_epoch(model, train_loader, loss_fn, optimizer, DEVICE)\n    print(f'Train loss: {train_loss:.4f}')\n\n    val_loss = eval_model(model, val_loader, loss_fn, DEVICE)\n    print(f'Validation loss: {val_loss:.4f}')\n\n    # Save the model if it has the best validation loss so far\n    if val_loss < best_val_loss:\n        torch.save(model.state_dict(), 'best_model_state.bin')\n        best_val_loss = val_loss\n\nprint(\"\\nTraining finished!\")\nprint(f\"Best validation loss: {best_val_loss:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-25T08:28:58.361713Z","iopub.execute_input":"2025-07-25T08:28:58.361928Z","iopub.status.idle":"2025-07-25T08:54:22.610213Z","shell.execute_reply.started":"2025-07-25T08:28:58.361911Z","shell.execute_reply":"2025-07-25T08:54:22.609508Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"701488a0bf33411fa7703711a33bdda7"}},"metadata":{}},{"name":"stdout","text":"Epoch 1/10\n----------\nTrain loss: 6.7535\nValidation loss: 1.9898\nEpoch 2/10\n----------\nTrain loss: 1.8180\nValidation loss: 1.0034\nEpoch 3/10\n----------\nTrain loss: 1.5868\nValidation loss: 1.0234\nEpoch 4/10\n----------\nTrain loss: 1.5444\nValidation loss: 0.9716\nEpoch 5/10\n----------\nTrain loss: 1.5426\nValidation loss: 1.0040\nEpoch 6/10\n----------\nTrain loss: 1.5018\nValidation loss: 0.9782\nEpoch 7/10\n----------\nTrain loss: 1.4943\nValidation loss: 0.9753\nEpoch 8/10\n----------\nTrain loss: 1.4474\nValidation loss: 1.0067\nEpoch 9/10\n----------\nTrain loss: 1.4702\nValidation loss: 0.9762\nEpoch 10/10\n----------\nTrain loss: 1.4465\nValidation loss: 0.9745\n\nTraining finished!\nBest validation loss: 0.9716\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}